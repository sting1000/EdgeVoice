#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å®æ—¶æµå¼è¯­éŸ³è¯†åˆ«æ¨ç†ç³»ç»Ÿ
æ”¯æŒéƒ¨ç½²ä¼˜åŒ–çš„Conformeræ¨¡å‹ï¼Œæ»¡è¶³limits.mdçº¦æŸ
"""

import os
import time
import threading
import queue
import numpy as np
import torch
import torch.nn.functional as F
import sounddevice as sd
import argparse
from collections import deque

from config import *
from models.streaming_conformer import StreamingConformer
from utils.feature_extraction import streaming_feature_extractor

class StreamingInferenceEngine:
    """æµå¼æ¨ç†å¼•æ“ï¼Œæ”¯æŒå®æ—¶éŸ³é¢‘å¤„ç†"""
    
    def __init__(self, model_path, device='cpu', confidence_threshold=0.8,
                 min_confidence_frames=3, sample_rate=TARGET_SAMPLE_RATE):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            device: è®¡ç®—è®¾å¤‡ ('cpu', 'cuda')
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            min_confidence_frames: æœ€å°è¿ç»­é«˜ç½®ä¿¡åº¦å¸§æ•°
            sample_rate: éŸ³é¢‘é‡‡æ ·ç‡
        """
        self.device = torch.device(device)
        self.confidence_threshold = confidence_threshold
        self.min_confidence_frames = min_confidence_frames
        self.sample_rate = sample_rate
        
        # åŠ è½½æ¨¡å‹
        self.model = self._load_model(model_path)
        print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æµå¼çŠ¶æ€
        self.reset_state()
        
        # éŸ³é¢‘ç¼“å†²åŒº
        self.audio_buffer = deque(maxlen=int(sample_rate * 2))  # 2ç§’ç¼“å†²
        self.is_recording = False
        
        # ç»“æœå¹³æ»‘
        self.recent_predictions = deque(maxlen=self.min_confidence_frames)
        
    def _load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            
        # åˆ›å»ºæ¨¡å‹å®ä¾‹ - ä½¿ç”¨éƒ¨ç½²ä¼˜åŒ–çš„å‚æ•°
        model = StreamingConformer(
            input_dim=N_MFCC * (2 * CONTEXT_FRAMES + 1),  # MFCC + ä¸Šä¸‹æ–‡
            hidden_dim=CONFORMER_HIDDEN_SIZE,
            num_classes=len(INTENT_CLASSES),
            num_layers=CONFORMER_LAYERS,
            num_heads=CONFORMER_ATTENTION_HEADS,
            dropout=0.0,  # æ¨ç†æ—¶ä¸ä½¿ç”¨dropout
            kernel_size=CONFORMER_CONV_KERNEL_SIZE,
            expansion_factor=CONFORMER_FF_EXPANSION_FACTOR
        )
        
        # åŠ è½½æƒé‡
        checkpoint = torch.load(model_path, map_location=self.device)
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
            
        model.to(self.device)
        model.eval()
        
        return model
    
    def reset_state(self):
        """é‡ç½®æµå¼æ¨ç†çŠ¶æ€"""
        self.model.reset_streaming_state()
        self.cached_states = None
        self.recent_predictions.clear()
        print("æµå¼çŠ¶æ€å·²é‡ç½®")
        
    def preprocess_audio_chunk(self, audio_chunk):
        """
        é¢„å¤„ç†éŸ³é¢‘å—ï¼Œæå–ç‰¹å¾
        
        Args:
            audio_chunk: éŸ³é¢‘æ•°æ® (numpyæ•°ç»„)
            
        Returns:
            features: ç‰¹å¾å¼ é‡ [1, seq_len, feature_dim]
        """
        try:
            # ä½¿ç”¨æµå¼ç‰¹å¾æå–å™¨
            features = streaming_feature_extractor(
                audio_chunk,
                sr=self.sample_rate,
                chunk_size=STREAMING_CHUNK_SIZE,
                step_size=STREAMING_STEP_SIZE
            )
            
            if features is None or len(features) == 0:
                return None
                
            # è½¬æ¢ä¸ºå¼ é‡å¹¶æ·»åŠ batchç»´åº¦
            features_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.device)
            
            return features_tensor
            
        except Exception as e:
            print(f"ç‰¹å¾æå–å¤±è´¥: {e}")
            return None
    
    def predict_streaming(self, features):
        """
        æ‰§è¡Œæµå¼é¢„æµ‹
        
        Args:
            features: è¾“å…¥ç‰¹å¾ [1, seq_len, feature_dim]
            
        Returns:
            prediction: é¢„æµ‹ç±»åˆ«
            confidence: ç½®ä¿¡åº¦
            intent_name: æ„å›¾åç§°
        """
        try:
            with torch.no_grad():
                # æµå¼æ¨ç†
                pred, conf, new_cached_states = self.model.predict_streaming(
                    features, self.cached_states
                )
                
                # æ›´æ–°ç¼“å­˜çŠ¶æ€
                self.cached_states = new_cached_states
                
                # è·å–é¢„æµ‹ç»“æœ
                prediction = pred.item()
                confidence = conf.item()
                intent_name = INTENT_CLASSES[prediction]
                
                return prediction, confidence, intent_name
                
        except Exception as e:
            print(f"æ¨ç†å¤±è´¥: {e}")
            return -1, 0.0, "ERROR"
    
    def smooth_predictions(self, prediction, confidence, intent_name):
        """
        å¹³æ»‘é¢„æµ‹ç»“æœï¼Œé¿å…æŠ–åŠ¨
        
        Args:
            prediction: å½“å‰é¢„æµ‹
            confidence: å½“å‰ç½®ä¿¡åº¦
            intent_name: å½“å‰æ„å›¾åç§°
            
        Returns:
            final_prediction: æœ€ç»ˆé¢„æµ‹ç»“æœ (Noneå¦‚æœè¿˜æœªç¨³å®š)
        """
        # åªè€ƒè™‘é«˜ç½®ä¿¡åº¦çš„é¢„æµ‹
        if confidence >= self.confidence_threshold:
            self.recent_predictions.append((prediction, confidence, intent_name))
        else:
            self.recent_predictions.append(None)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿ç»­çš„é«˜ç½®ä¿¡åº¦é¢„æµ‹
        if len(self.recent_predictions) >= self.min_confidence_frames:
            # æ£€æŸ¥æœ€è¿‘çš„é¢„æµ‹æ˜¯å¦ä¸€è‡´ä¸”é«˜ç½®ä¿¡åº¦
            recent_valid = [p for p in self.recent_predictions if p is not None]
            
            if len(recent_valid) >= self.min_confidence_frames:
                # æ£€æŸ¥é¢„æµ‹ä¸€è‡´æ€§
                recent_preds = [p[0] for p in recent_valid[-self.min_confidence_frames:]]
                if len(set(recent_preds)) == 1:  # æ‰€æœ‰é¢„æµ‹éƒ½ç›¸åŒ
                    final_pred = recent_preds[0]
                    avg_conf = np.mean([p[1] for p in recent_valid[-self.min_confidence_frames:]])
                    intent = recent_valid[-1][2]
                    return final_pred, avg_conf, intent
        
        return None
    
    def process_audio_chunk(self, audio_chunk):
        """
        å¤„ç†å•ä¸ªéŸ³é¢‘å—
        
        Args:
            audio_chunk: éŸ³é¢‘æ•°æ®
            
        Returns:
            result: è¯†åˆ«ç»“æœå­—å…¸æˆ–None
        """
        # é¢„å¤„ç†
        features = self.preprocess_audio_chunk(audio_chunk)
        if features is None:
            return None
            
        # æ¨ç†
        prediction, confidence, intent_name = self.predict_streaming(features)
        if prediction == -1:
            return None
            
        # å¹³æ»‘
        smoothed_result = self.smooth_predictions(prediction, confidence, intent_name)
        
        if smoothed_result is not None:
            final_pred, avg_conf, final_intent = smoothed_result
            return {
                'intent': final_intent,
                'confidence': avg_conf,
                'timestamp': time.time(),
                'prediction_id': final_pred
            }
        
        return None

class RealTimeAudioProcessor:
    """å®æ—¶éŸ³é¢‘å¤„ç†å™¨"""
    
    def __init__(self, inference_engine, chunk_duration=0.1):
        """
        åˆå§‹åŒ–éŸ³é¢‘å¤„ç†å™¨
        
        Args:
            inference_engine: æ¨ç†å¼•æ“å®ä¾‹
            chunk_duration: éŸ³é¢‘å—æŒç»­æ—¶é—´(ç§’)
        """
        self.inference_engine = inference_engine
        self.chunk_duration = chunk_duration
        self.chunk_size = int(TARGET_SAMPLE_RATE * chunk_duration)
        
        self.audio_queue = queue.Queue()
        self.result_queue = queue.Queue()
        self.is_running = False
        
        self.audio_buffer = np.array([])
        
    def audio_callback(self, indata, frames, time, status):
        """éŸ³é¢‘è¾“å…¥å›è°ƒå‡½æ•°"""
        if status:
            print(f"éŸ³é¢‘è¾“å…¥çŠ¶æ€: {status}")
            
        # å°†éŸ³é¢‘æ•°æ®åŠ å…¥é˜Ÿåˆ—
        audio_data = indata[:, 0]  # å–å•å£°é“
        self.audio_queue.put(audio_data.copy())
    
    def start_recording(self):
        """å¼€å§‹å½•éŸ³"""
        self.is_running = True
        
        # å¯åŠ¨éŸ³é¢‘æµ
        self.audio_stream = sd.InputStream(
            samplerate=TARGET_SAMPLE_RATE,
            channels=1,
            dtype=np.float32,
            blocksize=self.chunk_size,
            callback=self.audio_callback
        )
        
        # å¯åŠ¨å¤„ç†çº¿ç¨‹
        self.processing_thread = threading.Thread(target=self._process_audio_loop)
        self.processing_thread.daemon = True
        
        self.audio_stream.start()
        self.processing_thread.start()
        
        print(f"å¼€å§‹å®æ—¶å½•éŸ³ï¼Œé‡‡æ ·ç‡: {TARGET_SAMPLE_RATE}Hz")
        print(f"éŸ³é¢‘å—å¤§å°: {self.chunk_size}æ ·æœ¬ ({self.chunk_duration*1000:.1f}ms)")
        
    def stop_recording(self):
        """åœæ­¢å½•éŸ³"""
        self.is_running = False
        
        if hasattr(self, 'audio_stream'):
            self.audio_stream.stop()
            self.audio_stream.close()
            
        if hasattr(self, 'processing_thread'):
            self.processing_thread.join(timeout=1.0)
        
        print("å½•éŸ³å·²åœæ­¢")
    
    def _process_audio_loop(self):
        """éŸ³é¢‘å¤„ç†ä¸»å¾ªç¯"""
        while self.is_running:
            try:
                # è·å–éŸ³é¢‘æ•°æ® (éé˜»å¡ï¼Œè¶…æ—¶0.1ç§’)
                audio_chunk = self.audio_queue.get(timeout=0.1)
                
                # ç´¯ç§¯éŸ³é¢‘æ•°æ®
                self.audio_buffer = np.concatenate([self.audio_buffer, audio_chunk])
                
                # å½“ç´¯ç§¯è¶³å¤Ÿçš„æ•°æ®æ—¶è¿›è¡Œå¤„ç†
                process_size = int(TARGET_SAMPLE_RATE * 0.5)  # å¤„ç†0.5ç§’çš„æ•°æ®
                
                if len(self.audio_buffer) >= process_size:
                    # å¤„ç†éŸ³é¢‘
                    result = self.inference_engine.process_audio_chunk(
                        self.audio_buffer[:process_size]
                    )
                    
                    # ç§»é™¤å·²å¤„ç†çš„æ•°æ®ï¼Œä¿ç•™é‡å éƒ¨åˆ†
                    overlap_size = int(TARGET_SAMPLE_RATE * 0.2)  # ä¿ç•™0.2ç§’é‡å 
                    self.audio_buffer = self.audio_buffer[process_size-overlap_size:]
                    
                    # å¦‚æœæœ‰ç»“æœï¼ŒåŠ å…¥ç»“æœé˜Ÿåˆ—
                    if result is not None:
                        self.result_queue.put(result)
                        
            except queue.Empty:
                continue
            except Exception as e:
                print(f"éŸ³é¢‘å¤„ç†é”™è¯¯: {e}")
    
    def get_result(self):
        """è·å–è¯†åˆ«ç»“æœ (éé˜»å¡)"""
        try:
            return self.result_queue.get_nowait()
        except queue.Empty:
            return None

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å®æ—¶æµå¼è¯­éŸ³è¯†åˆ«')
    parser.add_argument('--model', '-m', required=True, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--device', '-d', default='cpu', choices=['cpu', 'cuda'], help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--confidence', '-c', type=float, default=0.8, help='ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--frames', '-f', type=int, default=3, help='æœ€å°è¿ç»­é«˜ç½®ä¿¡åº¦å¸§æ•°')
    parser.add_argument('--duration', '-t', type=int, default=30, help='å½•éŸ³æ—¶é•¿(ç§’ï¼Œ0ä¸ºæ— é™)')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("EdgeVoice å®æ—¶æµå¼è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ")
    print("=" * 60)
    print(f"æ¨¡å‹æ–‡ä»¶: {args.model}")
    print(f"è®¡ç®—è®¾å¤‡: {args.device}")
    print(f"ç½®ä¿¡åº¦é˜ˆå€¼: {args.confidence}")
    print(f"æœ€å°è¿ç»­å¸§æ•°: {args.frames}")
    print(f"æ„å›¾ç±»åˆ«: {', '.join(INTENT_CLASSES)}")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–æ¨ç†å¼•æ“
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        inference_engine = StreamingInferenceEngine(
            model_path=args.model,
            device=args.device,
            confidence_threshold=args.confidence,
            min_confidence_frames=args.frames
        )
        
        # åˆå§‹åŒ–éŸ³é¢‘å¤„ç†å™¨
        audio_processor = RealTimeAudioProcessor(inference_engine)
        
        # å¼€å§‹å½•éŸ³
        audio_processor.start_recording()
        
        print("\nğŸ¤ å¼€å§‹å®æ—¶è¯­éŸ³è¯†åˆ«...")
        print("è¯´å‡ºä»¥ä¸‹å‘½ä»¤ä¹‹ä¸€:")
        for i, intent in enumerate(INTENT_CLASSES):
            print(f"  {i+1}. {intent}")
        print("\næŒ‰ Ctrl+C åœæ­¢\n")
        
        start_time = time.time()
        
        # ä¸»å¾ªç¯
        while True:
            # æ£€æŸ¥å½•éŸ³æ—¶é•¿
            if args.duration > 0 and time.time() - start_time > args.duration:
                print(f"\nå½•éŸ³æ—¶é•¿è¾¾åˆ° {args.duration} ç§’ï¼Œè‡ªåŠ¨åœæ­¢")
                break
                
            # è·å–è¯†åˆ«ç»“æœ
            result = audio_processor.get_result()
            if result is not None:
                timestamp = time.strftime("%H:%M:%S", time.localtime(result['timestamp']))
                print(f"[{timestamp}] ğŸ¯ è¯†åˆ«ç»“æœ: {result['intent']} "
                      f"(ç½®ä¿¡åº¦: {result['confidence']:.3f})")
                
                # å¦‚æœæ˜¯åœæ­¢å½•éŸ³å‘½ä»¤ï¼Œè‡ªåŠ¨é€€å‡º
                if result['intent'] == 'STOP_RECORDING':
                    print("æ£€æµ‹åˆ°åœæ­¢å½•éŸ³å‘½ä»¤ï¼Œæ­£åœ¨é€€å‡º...")
                    break
            
            time.sleep(0.05)  # 50msæ£€æŸ¥é—´éš”
            
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...")
    except Exception as e:
        print(f"\né”™è¯¯: {e}")
    finally:
        # æ¸…ç†èµ„æº
        if 'audio_processor' in locals():
            audio_processor.stop_recording()
        print("ç¨‹åºå·²é€€å‡º")

if __name__ == "__main__":
    main() 