# EdgeVoice音频意图识别流式处理文档

## 1. 总体流程

EdgeVoice系统采用流式处理方式进行音频意图识别，整个流程如下：

1. 音频数据采集
2. 预处理（重采样、VAD语音活动检测）
3. 特征提取（MFCC特征及其差分）
4. 流式推理（基于Conformer架构）
5. 意图识别结果输出

## 2. 音频参数配置

系统采用以下音频参数配置：

| 参数 | 值 | 说明 |
|------|-----|------|
| 原始采样率 | 48000Hz | 设备采集的原始音频采样率 |
| 目标采样率 | 16000Hz | 模型处理的采样率 |
| 帧长 | 25ms | 每帧音频的时长 |
| 帧移 | 10ms | 相邻帧之间的偏移 |
| 最大命令长度 | 5秒 | 单个语音命令的最大时长 |

## 3. VAD语音活动检测

VAD用于检测音频中的语音段，关键参数如下：

| 参数 | 值 | 说明 |
|------|-----|------|
| 能量阈值 | 0.05 | 判断语音段的能量阈值 |
| 过零率阈值 | 0.15 | 判断语音段的过零率阈值 |
| 最小语音时长 | 100ms | 有效语音段的最小持续时间 |
| 最小静音时长 | 300ms | 判断语音结束的最小静音时间 |

VAD算法基于能量和过零率，实现在`vad_detect()`函数中：
1. 计算当前音频块的能量均值
2. 计算过零率
3. 当能量或过零率超过阈值时，判定为语音
4. 维护语音段状态，当检测到连续语音帧数超过阈值时触发语音开始
5. 当检测到连续静音帧数超过阈值时触发语音结束

## 4. 特征提取流程

系统使用MFCC特征及其一阶和二阶差分特征，参数配置如下：

| 参数 | 值 | 说明 |
|------|-----|------|
| MFCC系数数量 | 16 | 每帧提取的MFCC系数数量 |
| FFT点数 | 400 | 傅里叶变换点数 (25ms*16000Hz) |
| 帧移采样点数 | 160 | 相邻帧移动的采样点数 (10ms*16000Hz) |
| 上下文帧数 | 2 | 特征扩展时包含的上下文帧数 |

流式特征提取主要通过`extract_features_streaming()`函数实现：
1. 将当前音频块与之前保存的上下文帧拼接
2. 提取MFCC特征
3. 计算一阶差分和二阶差分特征
4. 去除上下文帧对应的特征
5. 保存当前块的末尾帧作为下一块的上下文

## 5. 流式处理和意图识别

系统使用块式流处理方式，参数如下：

| 参数 | 值 | 说明 |
|------|-----|------|
| 处理块大小 | 10帧 | 每次处理的帧数 (10*10ms=100ms) |
| 处理步长 | 5帧 | 相邻处理块的步长 (5*10ms=50ms) |
| 信心阈值 | 0.9 | 早停的置信度阈值 |
| 缓冲块大小 | 0.2秒 | 实时处理中的缓冲区大小 |

流式处理过程（`RealTimeStreamingProcessor`类）：
1. 接收音频块，通过VAD检测是否有语音
2. 当检测到语音时，开始积累音频数据到缓冲区
3. 提取特征并进行流式推理
4. 通过`predict_streaming`方法获取当前意图预测和置信度
5. 如果置信度高于阈值，可以提前输出结果（早停）
6. 当检测到语音结束时，输出最终结果并重置状态

## 6. 模型架构

系统使用基于Conformer的快速意图分类器，主要参数如下：

| 参数 | 值 | 说明 |
|------|-----|------|
| Conformer层数 | 4 | 模型的Conformer块数量 |
| 注意力头数 | 8 | 多头注意力机制中的头数 |
| 隐藏层大小 | 128 | 模型隐藏层维度 |
| 卷积核大小 | 31 | Conformer中卷积层的核大小 |
| Dropout比例 | 0.2 | 防止过拟合的Dropout率 |

流式推理过程：
1. 将特征输入模型前向传播
2. Conformer块处理特征并维护缓存状态
3. 对当前块进行全局平均池化
4. 通过全连接层获取分类结果
5. 返回预测类别、置信度和更新后的缓存状态

## 7. 时延情况

系统的时延主要包括以下几部分：

1. **音频采集延迟**：约50-100ms（根据缓冲块大小设置）
2. **预处理时延**：
   - 重采样：5-10ms
   - VAD检测：2-5ms
3. **特征提取时延**：10-20ms
4. **模型推理时延**：
   - 快速分类器：15-30ms（CPU）/ 5-10ms（GPU）
   - 精确分类器（可选）：额外30-50ms
5. **后处理时延**：1-3ms

**端到端时延情况**：

| 场景 | 总时延 | 说明 |
|------|--------|------|
| 最佳情况（早停） | 80-120ms | 高置信度情况下提前输出结果 |
| 普通情况 | 200-300ms | 完整处理一个语音命令 |
| 最差情况 | 400-500ms | 包含精确分类器的完整处理 |

系统会在特定意图（如"TAKE_PHOTO"和"GET_BATTERY_LEVEL"）的置信度高时启用早停机制，可显著减少处理时延。

## 8. 部署配置

在实际部署中，系统支持以下参数调整以平衡性能和延迟：

1. 调整VAD参数（能量阈值、过零率阈值）以适应不同噪声环境
2. 修改早停置信度阈值以平衡准确率和响应速度
3. 通过修改处理块大小和步长调整时延和计算量

## 9. 总结

EdgeVoice的端到端流式处理系统采用了高效的流式架构，通过VAD、流式特征提取和基于Conformer的模型，实现了低延迟的音频意图识别。系统能够在100-300ms的延迟内完成从音频输入到意图识别的全流程，并通过早停机制进一步优化时延表现。
